#!/usr/bin/env python3
"""
Test du vrai LLM Ollama avec vos donn√©es r√©elles Hutchinson
"""

from rag_with_llm import RegulatoryRiskRAGWithLLM
from db import db
from datetime import datetime
import requests
import time

def test_ollama_with_real_hutchinson_data():
    """Teste Ollama avec vos vraies donn√©es MongoDB"""

    print("ü¶ô TEST OLLAMA LLM AVEC DONN√âES R√âELLES HUTCHINSON")
    print("=" * 70)

    # √âtape 1: V√©rifier qu'Ollama et llama2 sont pr√™ts
    print("1Ô∏è‚É£ V√©rification d'Ollama et llama2...")

    for attempt in range(10):  # Attendre jusqu'√† 30 secondes
        try:
            # V√©rifier qu'Ollama r√©pond
            response = requests.get("http://localhost:11434/api/tags", timeout=3)
            if response.status_code == 200:
                models_data = response.json()
                models = [m.get('name', 'N/A') for m in models_data.get('models', [])]

                if 'llama2' in str(models) or any('llama2' in str(m) for m in models):
                    print("‚úÖ Ollama et llama2 sont pr√™ts !")
                    print(f"üìã Mod√®les disponibles: {models}")
                    break
                else:
                    print(f"‚è≥ Tentative {attempt + 1}/10 - llama2 en cours de t√©l√©chargement...")
                    print(f"   Mod√®les actuels: {models}")
                    time.sleep(3)
            else:
                print(f"‚ö†Ô∏è Ollama r√©pond avec erreur {response.status_code}")
                time.sleep(3)
        except requests.exceptions.ConnectionError:
            print(f"‚è≥ Tentative {attempt + 1}/10 - Ollama en cours de d√©marrage...")
            time.sleep(3)
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            time.sleep(3)
    else:
        print("‚ùå Timeout - llama2 n'est pas encore pr√™t")
        print("üí° V√©rifiez manuellement avec: ollama list")
        return False

    # √âtape 2: R√©cup√©rer vos vraies donn√©es
    print(f"\n2Ô∏è‚É£ R√©cup√©ration de vos donn√©es r√©elles...")

    try:
        regulations = list(db.regulations.find())
        hutchinson = db.hutchinson.find_one()

        if not regulations:
            print("‚ùå Aucune r√©glementation trouv√©e dans MongoDB")
            return False

        if not hutchinson:
            print("‚ùå Profil Hutchinson non trouv√© dans MongoDB")
            return False

        print(f"‚úÖ {len(regulations)} r√©glementations r√©cup√©r√©es")
        print(f"‚úÖ Profil Hutchinson r√©cup√©r√©")

        # Afficher les r√©glementations qui seront analys√©es
        print(f"\nüìã R√©glementations √† analyser par llama2:")
        for i, reg in enumerate(regulations, 1):
            print(f"   {i}. {reg.get('nom_loi', 'Nom non d√©fini')}")

    except Exception as e:
        print(f"‚ùå Erreur r√©cup√©ration donn√©es: {e}")
        return False

    # √âtape 3: Initialiser le syst√®me avec Ollama
    print(f"\n3Ô∏è‚É£ Initialisation du syst√®me RAG avec Ollama...")

    try:
        rag_ollama = RegulatoryRiskRAGWithLLM("ollama")  # Mode Ollama
        print("‚úÖ Syst√®me RAG + Ollama initialis√©")
    except Exception as e:
        print(f"‚ùå Erreur initialisation RAG: {e}")
        return False

    # √âtape 4: Pr√©parer les donn√©es pour llama2
    print(f"\n4Ô∏è‚É£ Pr√©paration des donn√©es pour llama2...")

    # Calculer les scores d'impact avec vos donn√©es
    high_risks = []
    medium_risks = []

    for reg in regulations:
        # Score d'impact bas√© sur Hutchinson
        impact_score = calculate_hutchinson_impact_score(reg, hutchinson)

        risk_entry = {
            "regulation": reg,
            "impact_details": [f"impact_score:{impact_score:.3f}"]
        }

        if impact_score >= 0.5:
            high_risks.append(risk_entry)
        elif impact_score >= 0.3:
            medium_risks.append(risk_entry)

    # Cr√©er le rapport pour llama2
    real_report = {
        "detailed_analysis": {
            "high_risk": high_risks,
            "medium_risk": medium_risks
        }
    }

    # Profil Hutchinson pour llama2
    hutchinson_profile = {
        "nom": hutchinson.get('company_info', {}).get('name', 'Hutchinson'),
        "secteur": ', '.join(hutchinson.get('company_info', {}).get('sectors', [])),
        "presence_geographique": hutchinson.get('geographical_presence', [])[:5],
        "matieres_premieres": hutchinson.get('typical_materials', [])[:5],
        "fournisseurs_regions": hutchinson.get('geographical_presence', [])[:3]
    }

    print(f"‚úÖ Donn√©es pr√©par√©es:")
    print(f"   ‚Ä¢ Risques √©lev√©s: {len(high_risks)}")
    print(f"   ‚Ä¢ Risques moyens: {len(medium_risks)}")
    print(f"   ‚Ä¢ Profil: {hutchinson_profile['nom']}")

    # √âtape 5: G√©n√©ration avec llama2
    print(f"\n5Ô∏è‚É£ ü¶ô G√âN√âRATION AVEC LLAMA2 LLM...")
    print("   (Cela peut prendre 30-120 secondes selon la complexit√©...)")

    try:
        start_time = time.time()

        # Appel du vrai LLM llama2
        llm_analysis = rag_ollama.generate_llm_analysis(real_report, hutchinson_profile)

        end_time = time.time()
        generation_time = end_time - start_time

        print(f"‚è±Ô∏è Temps de g√©n√©ration llama2: {generation_time:.1f} secondes")

        if "Erreur" in str(llm_analysis):
            print(f"‚ùå Erreur llama2: {llm_analysis}")
            return False

        print(f"‚úÖ SUCC√àS ! llama2 a g√©n√©r√© l'analyse")
        print(f"üìÑ Taille de la r√©ponse: {len(str(llm_analysis))} caract√®res")

        # Afficher l'analyse de llama2
        print(f"\n" + "="*80)
        print("ü¶ô ANALYSE G√âN√âR√âE PAR LLAMA2 LLM AVEC VOS DONN√âES R√âELLES:")
        print("="*80)
        print(llm_analysis)
        print("="*80)

        # Sauvegarder l'analyse
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"hutchinson_llama2_real_analysis_{timestamp}.txt"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write("ANALYSE HUTCHINSON PAR LLAMA2 LLM - DONN√âES R√âELLES\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M')}\n")
            f.write(f"Mod√®le LLM: llama2\n")
            f.write(f"Temps de g√©n√©ration: {generation_time:.1f} secondes\n")
            f.write(f"R√©glementations analys√©es: {len(regulations)}\n")
            f.write(f"Risques √©lev√©s d√©tect√©s: {len(high_risks)}\n")
            f.write(f"Risques moyens d√©tect√©s: {len(medium_risks)}\n\n")
            f.write("PROFIL HUTCHINSON:\n")
            f.write(f"Nom: {hutchinson_profile['nom']}\n")
            f.write(f"Secteurs: {hutchinson_profile['secteur']}\n")
            f.write(f"G√©ographie: {hutchinson_profile['presence_geographique']}\n\n")
            f.write("ANALYSE LLAMA2:\n")
            f.write("-" * 60 + "\n")
            f.write(str(llm_analysis))

        print(f"\nüíæ Analyse llama2 sauv√©e: {filename}")

        # Comparer avec l'analyse par r√®gles
        print(f"\n6Ô∏è‚É£ Comparaison avec l'analyse par r√®gles...")

        rag_rules = RegulatoryRiskRAGWithLLM("rules")
        rules_analysis = rag_rules.generate_llm_analysis(real_report, hutchinson_profile)

        print(f"üìä Comparaison:")
        print(f"   ‚Ä¢ llama2 LLM: {len(str(llm_analysis))} caract√®res")
        print(f"   ‚Ä¢ Syst√®me r√®gles: {len(str(rules_analysis))} caract√®res")

        return True

    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration llama2: {e}")
        import traceback
        traceback.print_exc()
        return False

def calculate_hutchinson_impact_score(regulation, hutchinson_profile):
    """Calcule le score d'impact pour Hutchinson"""
    score = 0.0

    # Score g√©ographique
    hutch_geo = hutchinson_profile.get('geographical_presence', [])
    reg_countries = regulation.get('pays_concernes', [])
    if hutch_geo and reg_countries:
        geo_matches = len(set(hutch_geo) & set(reg_countries))
        score += min(0.4, (geo_matches / len(reg_countries)) * 0.4)

    # Score sectoriel
    hutch_sectors = [s.lower() for s in hutchinson_profile.get('company_info', {}).get('sectors', [])]
    reg_sectors = [s.lower() for s in regulation.get('secteurs', [])]
    if hutch_sectors and reg_sectors:
        sector_matches = len(set(hutch_sectors) & set(reg_sectors))
        score += min(0.35, (sector_matches / len(reg_sectors)) * 0.35)

    # Score mots-cl√©s
    reg_text = regulation.get('texte', '').lower()
    hutch_keywords = ['sealing', 'automotive', 'aerospace', 'rubber', 'steel', 'vibration']
    keyword_matches = sum(1 for keyword in hutch_keywords if keyword in reg_text)
    score += min(0.25, (keyword_matches / len(hutch_keywords)) * 0.25)

    return min(1.0, score)

if __name__ == "__main__":
    print("üöÄ TEST DU VRAI LLM LLAMA2 AVEC HUTCHINSON")
    print("Ce test utilise le mod√®le llama2 que vous venez d'installer")
    print("pour analyser vos vraies donn√©es r√©glementaires.")
    print("\n" + "="*70)

    success = test_ollama_with_real_hutchinson_data()

    if success:
        print(f"\nüéâ SUCC√àS COMPLET !")
        print("ü¶ô llama2 LLM fonctionne parfaitement avec vos donn√©es")
        print("üéØ Votre syst√®me utilise maintenant un VRAI LLM pour l'analyse")
        print("\nüí° Pour utiliser llama2 dans vos analyses:")
        print("   rag = RegulatoryRiskRAGWithLLM('ollama')")
        print("   # Le syst√®me utilisera automatiquement llama2")
    else:
        print(f"\n‚ùå PROBL√àME D√âTECT√â")
        print("üí° V√©rifications possibles:")
        print("   ‚Ä¢ ollama list (v√©rifier que llama2 est t√©l√©charg√©)")
        print("   ‚Ä¢ ollama run llama2 (tester llama2 manuellement)")
        print("   ‚Ä¢ curl http://localhost:11434/api/tags (v√©rifier Ollama)")
