from vector_search import search_similar_regulations
from db import db
import json
from datetime import datetime

def get_hutchinson_profile():
    """R√©cup√®re le profil Hutchinson depuis la base"""
    hutchinson = db["hutchinson"]
    profile = hutchinson.find_one({})
    return profile

def create_analysis_prompt(query, regulations, hutchinson_profile):
    """
    Cr√©e un prompt intelligent pour que le LLM analyse les r√©glementations
    de mani√®re autonome pour Hutchinson
    """

    # Construire le contexte Hutchinson
    hutchinson_context = f"""
PROFIL ENTREPRISE - HUTCHINSON:
- Nom: {hutchinson_profile['company_info']['name']}
- Secteurs: {', '.join(hutchinson_profile['company_info']['sectors'])}
- Si√®ge: {hutchinson_profile['company_info']['headquarters']}
- Activit√© principale: {hutchinson_profile['company_info']['main_business']}

Pr√©sence g√©ographique: {', '.join(hutchinson_profile['geographical_presence'])}

Activit√©s par secteur:
"""

    for sector, activities in hutchinson_profile['business_activities'].items():
        hutchinson_context += f"- {sector}: {', '.join(activities)}\n"

    hutchinson_context += f"\nMat√©riaux utilis√©s: {', '.join(hutchinson_profile['typical_materials'])}"
    hutchinson_context += f"\nProduits sp√©cifiques: {', '.join(hutchinson_profile['specific_products'])}"

    # Construire le contexte des r√©glementations trouv√©es
    regulations_context = ""
    for i, reg in enumerate(regulations, 1):
        regulations_context += f"""
R√âGLEMENTATION {i}:
ID: {reg.get('id_loi', 'N/A')}
Titre: {reg.get('nom_loi', 'N/A')}
Type: {reg.get('type', 'N/A')}
Lien: {reg.get('lien_loi', 'N/A')}
Date publication: {reg.get('date_publication', 'N/A')}
Pays concern√©s: {reg.get('pays_concernes', [])}
Score de similarit√©: {reg.get('score', 0):.4f}

Contenu:
{reg.get('texte', '')[:1500]}...

---
"""

    # Prompt principal pour le LLM
    prompt = f"""Tu es un expert en conformit√© r√©glementaire sp√©cialis√© dans l'analyse d'impact pour les entreprises industrielles.

{hutchinson_context}

QUESTION DE L'UTILISATEUR: "{query}"

R√âGLEMENTATIONS TROUV√âES PAR RECHERCHE VECTORIELLE:
{regulations_context}

MISSION:
Analyse ces r√©glementations et d√©termine leur impact sur Hutchinson. Pour chaque r√©glementation, √©value:

1. PERTINENCE (0-10): Cette r√©glementation concerne-t-elle vraiment les activit√©s de Hutchinson ?
   - 0-3: Pas du tout pertinente (ex: pharmaceutique, banque)
   - 4-6: Moyennement pertinente 
   - 7-10: Tr√®s pertinente (secteurs auto/a√©ro, mat√©riaux, √©tanch√©it√©, vibration)

2. IMPACT G√âOGRAPHIQUE: Quels sites Hutchinson sont concern√©s ?

3. IMPACT M√âTIER: Quels secteurs/produits/mat√©riaux Hutchinson sont impact√©s ?

4. SANCTIONS: Quelles sont les sanctions pr√©vues (amendes, prison, suspension) ?

5. SCORE DE RISQUE (0-100): Niveau de risque global pour Hutchinson
   - 0-30: FAIBLE
   - 31-60: MOYEN  
   - 61-80: √âLEV√â
   - 81-100: CRITIQUE

6. RECOMMANDATIONS: Actions prioritaires √† prendre

FORMAT DE R√âPONSE:
Pour chaque r√©glementation, structure ta r√©ponse ainsi:

## R√âGLEMENTATION [ID]: [Titre]
**Pertinence:** [0-10] - [Explication]
**Impact g√©ographique:** [Liste des pays/sites concern√©s]
**Impact m√©tier:** [Secteurs/produits/mat√©riaux impact√©s]
**Sanctions d√©tect√©es:** [Liste des sanctions avec montants]
**Score de risque:** [0-100] - Niveau [FAIBLE/MOYEN/√âLEV√â/CRITIQUE]
**Recommandations:** [Actions prioritaires]

SYNTH√àSE GLOBALE:
- Nombre total de r√©glementations analys√©es: [X]
- R√©glementations critiques (score > 80): [X] 
- R√©glementations √† haut risque (score 61-80): [X]
- Actions prioritaires imm√©diates: [Liste]

Sois pr√©cis, factuel et base-toi uniquement sur les informations fournies."""

    return prompt

def analyze_regulations_with_llm(query, use_local_llm=True):
    """
    Analyse compl√®te des r√©glementations avec LLM autonome

    Args:
        query (str): Question de l'utilisateur
        use_local_llm (bool): Utiliser Ollama local ou API externe
    """

    print(f"üîç Recherche de r√©glementations pour: '{query}'")

    # √âTAPE 1: R√©cup√©ration via recherche vectorielle
    regulations = search_similar_regulations(query, limit=5)

    if not regulations:
        return "‚ùå Aucune r√©glementation pertinente trouv√©e."

    print(f"‚úÖ {len(regulations)} r√©glementations trouv√©es")

    # √âTAPE 2: R√©cup√©rer le profil Hutchinson
    hutchinson_profile = get_hutchinson_profile()
    if not hutchinson_profile:
        return "‚ùå Profil Hutchinson non trouv√© dans la base de donn√©es."

    # √âTAPE 3: Cr√©er le prompt intelligent
    prompt = create_analysis_prompt(query, regulations, hutchinson_profile)

    print("ü§ñ G√©n√©ration de l'analyse par le LLM...")

    # √âTAPE 4: Appeler le LLM
    if use_local_llm:
        analysis = call_ollama_llm(prompt)
    else:
        analysis = call_openai_llm(prompt)

    return analysis

def call_ollama_llm(prompt):
    """Appelle Ollama en local"""
    try:
        import requests

        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2",  # ou llama3.1, mistral, etc.
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # Plus factuel
                    "top_p": 0.9
                }
            },
            timeout=120  # 2 minutes max
        )

        if response.status_code == 200:
            result = response.json()
            return result.get("response", "Erreur: pas de r√©ponse du LLM")
        else:
            return f"‚ùå Erreur Ollama: {response.status_code}"

    except requests.exceptions.RequestException as e:
        return f"‚ùå Erreur de connexion Ollama: {e}\nüí° Assurez-vous qu'Ollama est d√©marr√© avec: ollama serve"
    except Exception as e:
        return f"‚ùå Erreur: {e}"

def call_openai_llm(prompt):
    """Appelle OpenAI API (n√©cessite cl√© API)"""
    try:
        import openai
        import os

        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            return "‚ùå Cl√© API OpenAI non configur√©e. D√©finissez OPENAI_API_KEY."

        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Tu es un expert en conformit√© r√©glementaire."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=4000
        )

        return response.choices[0].message.content

    except Exception as e:
        return f"‚ùå Erreur OpenAI: {e}"

def interactive_regulatory_consultant():
    """Interface interactive pour consultation r√©glementaire avec LLM"""

    print("ü§ñ CONSULTANT IA R√âGLEMENTAIRE - HUTCHINSON")
    print("üîã Aliment√© par recherche vectorielle + LLM autonome")
    print("=" * 60)

    print("Choisissez votre LLM:")
    print("1. Ollama (local, gratuit)")
    print("2. OpenAI GPT-4 (API, payant)")

    while True:
        choice = input("Votre choix (1 ou 2): ").strip()
        if choice in ["1", "2"]:
            use_local = (choice == "1")
            break
        print("‚ùå Veuillez choisir 1 ou 2")

    llm_name = "Ollama" if use_local else "OpenAI GPT-4"
    print(f"‚úÖ LLM s√©lectionn√©: {llm_name}")
    print("\nüí° Exemples de questions:")
    print("- 'r√©glementations automobiles √©tanch√©it√©'")
    print("- 'sanctions a√©rospatiale vibration'")
    print("- 'CBAM √©missions carbone'")
    print("- 'r√©glementations mat√©riaux caoutchouc'")
    print("\nüí° Tapez 'quit' pour quitter")
    print("=" * 60)

    while True:
        query = input("\n‚ùì Votre question: ").strip()

        if query.lower() in ['quit', 'exit', 'q']:
            print("üëã Au revoir !")
            break

        if not query:
            continue

        print("\n" + "‚è≥" * 20)
        print("üîÑ Analyse en cours...")

        try:
            analysis = analyze_regulations_with_llm(query, use_local_llm=use_local)

            print("\n" + "=" * 80)
            print("üìÑ RAPPORT D'ANALYSE R√âGLEMENTAIRE")
            print("=" * 80)
            print(analysis)
            print("=" * 80)

            # Sauvegarder le rapport
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rapport_reglementaire_{timestamp}.txt"

            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"RAPPORT D'ANALYSE R√âGLEMENTAIRE - {datetime.now()}\n")
                f.write(f"Question: {query}\n")
                f.write("=" * 80 + "\n")
                f.write(analysis)

            print(f"üíæ Rapport sauvegard√©: {filename}")

        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse: {e}")

if __name__ == "__main__":
    # Test rapide
    print("üß™ Test du syst√®me RAG + LLM")
    test_query = "r√©glementations automobiles syst√®mes √©tanch√©it√©"

    # Version simple pour test (sans LLM)
    regulations = search_similar_regulations(test_query, limit=3)
    print(f"‚úÖ Test recherche vectorielle: {len(regulations)} r√©sultats")

    # Lancer l'interface interactive
    print("\nüöÄ Lancement de l'interface interactive...")
    interactive_regulatory_consultant()
