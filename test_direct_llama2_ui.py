#!/usr/bin/env python3
"""
Test direct llama2 avec vos donnÃ©es MongoDB - Contournement du systÃ¨me de recherche
"""

from rag_with_llm import RegulatoryRiskRAGWithLLM
from db import db
from datetime import datetime
import json

def test_direct_llama2():
    """Test direct de llama2 en contournant le systÃ¨me de recherche"""

    print("ğŸ¦™ TEST DIRECT LLAMA2 AVEC VOS DONNÃ‰ES")
    print("=" * 60)

    try:
        # RÃ©cupÃ©rer directement vos donnÃ©es MongoDB
        regulations = list(db.regulations.find().limit(2))  # LimitÃ© Ã  2
        hutchinson = db.hutchinson.find_one()

        if not regulations:
            print("âŒ Aucune rÃ©glementation en base")
            return False

        if not hutchinson:
            print("âŒ Profil Hutchinson non trouvÃ©")
            return False

        print(f"âœ… {len(regulations)} rÃ©glementations rÃ©cupÃ©rÃ©es")
        print(f"âœ… Profil Hutchinson rÃ©cupÃ©rÃ©")

        # Afficher les rÃ©glementations qui seront analysÃ©es
        print(f"\nğŸ“‹ RÃ©glementations Ã  analyser par llama2:")
        for i, reg in enumerate(regulations, 1):
            deadline = reg.get('date_effet') or reg.get('date_vigueur')
            deadline_str = deadline.strftime('%d/%m/%Y') if deadline else "Non dÃ©finie"
            print(f"   {i}. {reg.get('nom_loi')}")
            print(f"      ğŸ“… Deadline: {deadline_str}")

        # Initialiser le systÃ¨me LLM
        rag = RegulatoryRiskRAGWithLLM("ollama")

        # CrÃ©er un rapport simple directement
        fake_report = {
            "detailed_analysis": {
                "high_risk": [
                    {"regulation": regulations[0], "impact_details": ["test"]}
                ],
                "medium_risk": [
                    {"regulation": regulations[1], "impact_details": ["test"]}
                ] if len(regulations) > 1 else []
            }
        }

        # Profil Hutchinson
        hutchinson_profile = {
            "nom": "Groupe Hutchinson",
            "secteur": "Automotive, Aerospace, Industry",
            "presence_geographique": ["France", "Germany", "United States"]
        }

        print(f"\nğŸ¤– GÃ©nÃ©ration analyse llama2 (2 rÃ©glementations max)...")

        # Appeler directement llama2
        llm_response = rag.generate_llm_analysis(fake_report, hutchinson_profile)

        print(f"âœ… llama2 a rÃ©pondu !")
        print(f"ğŸ“„ Taille rÃ©ponse: {len(str(llm_response))} caractÃ¨res")

        # Afficher la rÃ©ponse brute de llama2
        print(f"\n" + "="*70)
        print("ğŸ¦™ RÃ‰PONSE BRUTE DE LLAMA2:")
        print("="*70)
        print(llm_response)
        print("="*70)

        # Essayer d'extraire les donnÃ©es UI
        try:
            ui_data = rag.extract_ui_data_from_llm_response(llm_response)

            if "error" not in ui_data:
                print(f"\nâœ… DONNÃ‰ES UI EXTRAITES AVEC SUCCÃˆS !")

                indicators = ui_data.get('indicators', [])
                print(f"ğŸ“Š {len(indicators)} indicateurs extraits:")

                for i, ind in enumerate(indicators, 1):
                    print(f"\n{i}. INDICATEUR POUR UI:")
                    print(f"   âœ… law_name: {ind.get('law_name', 'N/A')}")
                    print(f"   âœ… law_url: {ind.get('law_url', '#')}")
                    print(f"   âœ… deadline: {ind.get('deadline', 'N/A')}")
                    print(f"   âœ… impact_financial: {ind.get('impact_financial', 0)}/10")
                    print(f"   âœ… impact_reputation: {ind.get('impact_reputation', 0)}/10")
                    print(f"   âœ… impact_operational: {ind.get('impact_operational', 0)}/10")
                    print(f"   âœ… notes: {ind.get('notes', 'N/A')}")

                # Sauvegarder les donnÃ©es UI
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                ui_filename = f"hutchinson_ui_llama2_{timestamp}.json"

                with open(ui_filename, 'w', encoding='utf-8') as f:
                    json.dump(ui_data, f, indent=2, ensure_ascii=False, default=str)

                print(f"\nğŸ’¾ DonnÃ©es UI sauvÃ©es: {ui_filename}")

            else:
                print(f"âŒ Erreur extraction UI: {ui_data['error']}")
                print("ğŸ“‹ RÃ©ponse LLM brute (pour debug):")
                print(str(llm_response)[:500] + "...")

        except Exception as e:
            print(f"âŒ Erreur extraction: {e}")

        return True

    except Exception as e:
        print(f"âŒ Erreur test: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ¯ TEST DIRECT LLAMA2 POUR DONNÃ‰ES UI")
    print("Test optimisÃ© pour obtenir UNIQUEMENT les donnÃ©es UI que vous voulez")
    print("\n" + "="*60)

    success = test_direct_llama2()

    if success:
        print(f"\nğŸ‰ SUCCÃˆS ! llama2 gÃ©nÃ¨re les donnÃ©es pour votre UI")
        print("ğŸ“± DonnÃ©es JSON prÃªtes pour Streamlit")
    else:
        print(f"\nâŒ ProblÃ¨me - vÃ©rifiez que llama2 est bien dÃ©marrÃ©")
